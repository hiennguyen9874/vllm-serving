[project]
name = "vllm-serving"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10,<3.11"
dependencies = [
    "accelerate>=1.6.0",
    "qwen-vl-utils[decord]==0.0.8",
    "setuptools>=80.0.0",
    "torch==2.7.1",
    "torchvision==0.22.1",
    "transformers",
    "setuptools-scm>=8.3.1",
    "vllm==0.10.1",
    "triton>=3.3.1",
]

[tool.uv]
environments = [
    "sys_platform == 'linux'",
]
no-build-isolation = true

[tool.uv.sources]
torch = [
  { index = "pytorch-cpu", marker = "sys_platform != 'linux'" },
  { index = "pytorch-cu126", marker = "sys_platform == 'linux'" },
]
torchvision = [
  { index = "pytorch-cpu", marker = "sys_platform != 'linux'" },
  { index = "pytorch-cu126", marker = "sys_platform == 'linux'" },
]
onnx-graphsurgeon = { index = "nvidia"}
transformers = { git = "https://github.com/huggingface/transformers", rev = "v4.55.0" }
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.2/flash_attn-2.8.2+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl" }

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true

[[tool.uv.index]]
name = "nvidia"
url = "https://pypi.ngc.nvidia.com"
explicit = true

[dependency-groups]
dev = [
    "flash-attn>=2.8.2",
    "flashinfer-python>=0.3.1",
]
