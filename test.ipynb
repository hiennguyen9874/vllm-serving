{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='chatcmpl-0d7ec18f9d5f4b5abfc23b5d5c394d4e', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nOkay, so I need to write a short introduction to large language models. Hmm, where do I start? I remember that LLMs are a type of AI, but I\\'m not entirely sure about the specifics. Let me think... I know they\\'re used for generating text, like writing articles or answering questions. But how exactly do they work?\\n\\nI think they use something called neural networks. Maybe deep learning? Yeah, I\\'ve heard terms like transformers and tokenization before. So, these models process text by breaking it down into tokens, which could be words or parts of words. Then, they analyze patterns in large datasets to predict what comes next.\\n\\nWait, why are they called \"large\"? Oh, right, because they have a huge number of parameters. Parameters are like the variables the model adjusts during training to make accurate predictions. More parameters mean more complexity, allowing the model to capture intricate language patterns. But does that also mean they require a lot of computational power? I believe so, especially for training them on massive datasets.\\n\\nI should mention the applications. They\\'re used in chatbots, content creation, summarization, translation, and even coding. But there are challenges too. Ethical issues come to mind, like bias in the data they\\'re trained on. If the dataset has biased information, the model might produce biased outputs. Also, misinformation is a problem because these models can generate convincing but false statements.\\n\\nAnother point is their environmental impact. Training large models consumes a significant amount of energy, contributing to carbon emissions. That\\'s a big concern nowadays with the push towards sustainability.\\n\\nI should structure this introduction logically. Start with what LLMs are, their architecture (like transformers), how they process text, their size and computational needs, applications, and then touch on the challenges like bias, misinformation, and environmental impact.\\n\\nLet me see if I missed anything. Maybe include how they\\'ve evolved from earlier models, but that might be too detailed for a short intro. Also, perhaps mention transfer learning, where models can be adapted to different tasks without retraining from scratch.\\n\\nI should keep each section concise since it\\'s an introduction. Make sure the language is clear and accessible, avoiding too much jargon unless necessary. Okay, I think I have a good outline now.\\n</think>\\n\\n**Introduction to Large Language Models**\\n\\nLarge Language Models (LLMs) are advanced artificial intelligence systems designed to process and generate human-like text. Utilizing deep learning techniques, particularly transformer architectures, these models break down text into tokens—words or subwords—and analyze patterns from vast datasets to predict subsequent text elements. The term \"large\" refers to their extensive parameter count, enabling them to capture complex language nuances, though this also necessitates significant computational resources for training.\\n\\nLLMs find applications in various domains, including chatbots, content creation, summarization, translation, and coding. However, their use presents challenges. Issues such as bias from training data, potential dissemination of misinformation, and high environmental impact due to energy-intensive training processes are notable concerns. Despite these challenges, LLMs represent a powerful tool in natural language processing, offering versatile capabilities through transfer learning to adapt across different tasks efficiently.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1748530363, model='Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=645, prompt_tokens=15, total_tokens=660, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8003/v1\"\n",
    "# openai_api_base = \"http://localhost:23333/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"/nothink Give me a short introduction to large language models.\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    # top_k=20,\n",
    "    max_tokens=8192,\n",
    "    presence_penalty=1.5,\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}},\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
